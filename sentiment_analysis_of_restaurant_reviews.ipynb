{
  "cells": [
    {
      "metadata": {
        "_uuid": "93b7d96ea4a342b529d12cd15ebec38a3bf08661",
        "id": "M63WC1_OZzhS"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of Restaurant Reviews"
      ]
    },
    {
      "metadata": {
        "_uuid": "c09ee4f9185b743274b9cbc808c2a46edd72a83f",
        "id": "gMIZ2Gp-Zzhh"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset: [Restaurant_Reviews.tsv](https://www.kaggle.com/hj5992/restaurantreviews) is a dataset from Kaggle datasets which consists of 1000 reviews on a restaurant.\n",
        "\n",
        "To build a model to predict if review is positive or negative, following steps are performed.\n",
        "\n",
        "* Importing Dataset\n",
        "* Create a dataframe\n",
        "* Preprocessing Dataset\n",
        "* Vectorization\n",
        "* Training and Classification\n",
        "* Save th model as .pkl\n",
        "* Develop a testing framework using Heroku amd test the reviews manually"
      ]
    },
    {
      "metadata": {
        "_uuid": "0b2828b368ecc03ab374142073170c7f7492634e",
        "id": "Ix7494OTZzhk"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Dataset"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": false,
        "id": "K_ajp9huZzhl"
      },
      "cell_type": "markdown",
      "source": [
        "Importing the Restaurant Review dataset using pandas library."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "776c1d0ee74fc5508a4a91ec4f3556e107b8b455",
        "id": "dKYsuj5RZzhn"
      },
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "JP_Aj5rGZzhq"
      },
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e35ff1aefdc1a9a69887d85d802f0b4406d7dbaa",
        "id": "3zLGOqy-Zzhs"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Dataset"
      ]
    },
    {
      "metadata": {
        "_uuid": "aa144f3ed6026061078c88034d1f906180ea5453",
        "id": "ssdFVb-VZzhu"
      },
      "cell_type": "markdown",
      "source": [
        "Each review undergoes through a preprocessing step, where all the vague information is removed.\n",
        "\n",
        "* Removing the Stopwords, numeric and speacial charecters.\n",
        "* Normalizing each review using the approach of stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-QEz4tcWrbP",
        "outputId": "d6e9056f-66ab-4a2d-cade-ad2e62d497a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7d73a4db8fe837094ff5e40c04919bf6047d1f3c",
        "id": "eF0YangCZzhv"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range(0, 1000):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51cc3a8def8bb7eeb6508651b972efc753763aeb",
        "id": "iOiBM63nZzhy"
      },
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ]
    },
    {
      "metadata": {
        "_uuid": "484360e5b03c701261000dc5544a38efad4dfde8",
        "id": "RpmNqpAtZzhz"
      },
      "cell_type": "markdown",
      "source": [
        "From the cleaned dataset, potential features are extracted and are converted to numerical format. The vectorization techniques are used to convert textual data to numerical format. Using vectorization, a matrix is created where each column represents a feature and each row represents an individual review."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e5cc79ee441495e83613276c7dab35f0ac1ddd2",
        "id": "iEzZ69fUZzh0"
      },
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model using CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a606773ada1375cdabfa0fec62f4b976d74a5eb5",
        "id": "pSPk9ExwZzh1"
      },
      "cell_type": "markdown",
      "source": [
        "### Training and Classification"
      ]
    },
    {
      "metadata": {
        "_uuid": "703554381341fa910ae696b6d150df9e9879e5d2",
        "id": "eDadY5_DZzh2"
      },
      "cell_type": "markdown",
      "source": [
        "Further the data is splitted into training and testing set using Cross Validation technique. This data is used as input to classification algorithm.\n",
        "\n",
        "**Classification Algorithms:**\n",
        "\n",
        "Algorithms like Decision tree, Support Vector Machine, Logistic Regression, Naive Bayes were implemented and on comparing the evaluation metrics two of the algorithms gave better predictions than others.\n",
        "\n",
        "* Multinomial Naive Bayes\n",
        "* Bernoulli Naive Bayes\n",
        "* Logistic Regression"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2b105697aba2280f3e9d8f9f691c63aa31e5a38d",
        "id": "OVsx2ujtZzh3"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea86674ad05134e5b7962bffc63f173a0817accb",
        "id": "F5U9bbQzZzh5"
      },
      "cell_type": "markdown",
      "source": [
        "**Multinomial NB**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae32807740cd509e4353b65ac7b588ab7df3fc9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG6QuUZHZzh6",
        "outputId": "9fc61b37-3fec-4dcf-bcd4-8e4f9cdcd6b9"
      },
      "cell_type": "code",
      "source": [
        "# Multinomial NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=0.1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[119  33]\n",
            " [ 34 114]]\n",
            "\n",
            "\n",
            "Accuracy is  77.67 %\n",
            "Precision is  0.78\n",
            "Recall is  0.77\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fb1890535da8d5b47d13c517d9c8f502c74ff7ca",
        "id": "dx0o6HHOZzh7"
      },
      "cell_type": "markdown",
      "source": [
        "**Bernoulli NB**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3afba49eb67124353842111eec8a856b8b910822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFmbnOYXZzh8",
        "outputId": "8cf98c85-932a-401c-996a-e4754e9a5c0d"
      },
      "cell_type": "code",
      "source": [
        "# Bernoulli NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB(alpha=0.8)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[115  37]\n",
            " [ 32 116]]\n",
            "\n",
            "\n",
            "Accuracy is  77.0 %\n",
            "Precision is  0.76\n",
            "Recall is  0.78\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b764217c7a6ba8b7acd8dcf72abb3b890d338a33",
        "id": "JfNoRDc_Zzh9"
      },
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec65c31586cbcd91c8ece17895b067dd23e76adf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn0lxzc1Zzh-",
        "outputId": "5e036750-1a8a-4a28-a98b-216002add1e0"
      },
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn import linear_model\n",
        "classifier = linear_model.LogisticRegression(C=1.5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[125  27]\n",
            " [ 43 105]]\n",
            "\n",
            "\n",
            "Accuracy is  76.67 %\n",
            "Precision is  0.8\n",
            "Recall is  0.71\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1b4ab0c5a0882e69b332bb3c607fe620e972942d",
        "id": "rJdS5fFNZzh_"
      },
      "cell_type": "markdown",
      "source": [
        "### Analysis and Conclusion"
      ]
    },
    {
      "metadata": {
        "_uuid": "6be3a04b43930fc0da789bdf38de7ce25fa477e1",
        "id": "u5yqHvMXZzh_"
      },
      "cell_type": "markdown",
      "source": [
        "In this study, an attempt has been made to classify sentiment analysis for restaurant reviews using machine learning techniques. Two algorithms namely Multinomial Naive Bayes and Bernoulli Naive Bayes are implemented.\n",
        "\n",
        "Evaluation metrics used here are accuracy, precision and recall.\n",
        "\n",
        "Using Multinomial Naive Bayes,\n",
        "\n",
        "* Accuracy of prediction is 77.67%.\n",
        "* Precision of prediction is 0.78.\n",
        "* Recall of prediction is 0.77.\n",
        "\n",
        "Using Bernoulli Naive Bayes,\n",
        "\n",
        "* Accuracy of prediction is 77.0%.\n",
        "* Precision of prediction is 0.76.\n",
        "* Recall of prediction is 0.78.\n",
        "\n",
        "Using Logistic Regression,\n",
        "\n",
        "* Accuracy of prediction is 76.67%.\n",
        "* Precision of prediction is 0.8.\n",
        "* Recall of prediction is 0.71.\n",
        "\n",
        "From the above results, Multinomial Naive Bayes is slightly better method compared to Bernoulli Naive Bayes and Logistic Regression, with 77.67% accuracy which means the model built for the prediction of sentiment of the restaurant review gives 77.67% right prediction."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "sentiment-analysis-of-restaurant-reviews.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}